### 多层感知机概念

**问题背景：**在前面进行的都是线性的模型计算（之前预测连续值的线性回归模型与预测类型的`softmax`回归），但是线性是一个局限性很大的相关关系，那么我们怎么去建立非线性的模型？

#### 隐藏层

在之前的模型中使用的是仿射变换，通过单个仿射变换将输入直接映射到输出

> 仿射变换：一种保持“直线性”与“平行性”的几何变换，在此处特指线性变换+平移

以分类猫狗图片为例，根据某个位置的像素强度来改变图像描绘猫 / 狗的似然是不可取的，因为猫狗的图像倒置、位置变化依然不影响其是猫 / 狗，任何像素的重要性以复杂的方式取决于该像素的上下文（周围像素的值）

因此需要在数据中**建立一种额外的表示**（也许是预处理？）**来考虑特征之间的相互交互作用**，在此表示的基础上建立一个线性模型

加入隐藏层的目的：加入一个或多个隐藏层，使其能处理更普遍的函数关系类型

简单加入隐藏层：

​	将许多全连接层堆叠到一起，每一层都输出到上面的层直至生成最后的输出（其中前`L-1`层为表示，最后一层为线性预测器），这种架构称为**多层感知机**（MLP）

![b363ef470166d1499262e64a8f371e03](D:\电脑管家迁移文件\xwechat_files\wxid_q9of5bilc37f22_ee2e\temp\RWTemp\2025-11\9e20f478899dc29eb19741386f9343c8\b363ef470166d1499262e64a8f371e03.png)

该多层感知机有4个输入，3个输出，隐藏层5个隐藏单元，层数为2（只需要隐藏层与输出层的计算）

> 全连接层：也叫稠密层，神经网络最基本的层类型之一。这种层中每个输入神经元都与每个输出神经元相连

那么将两层的计算分别写出方程如下：
$$
H &= XW^{(1)} + b^{(1)}, \\
    O &= HW^{(2)} + b^{(2)}.
$$
两个方程中的W与b的形状与取值都不一样，H被称为“隐藏（层）变量”

**问题是**这两个线性模型在连接起来后，从输入到输出依然是仿射变换：也就是我们除了添加了一层两个参数的计算量之外，没有任何好处。解决这个问题的方法是添加激活函数

**激活函数：**在一层执行完线性计算后执行，根据该函数决定输出的神经元是否“激活”与激活程度。激活函数的输出被称为“活性值”。加入激活函数后，输入与输出大概率就不是仿射变换的关系了（注意，在最后的输出层是不需要激活函数的）
$$
H &= \sigma(XW^{(1)} + b^{(1)}), \\
    O &= HW^{(2)} + b^{(2)}.
$$

#### 激活函数

##### `ReLU`函数

又称为“修正线性单元”，提供一种非常简单的非线性变换
$$
ReLU(x) = max(x, 0)
$$
简单来说，仅保留了正元素并丢弃所有负元素

该激活函数有很多变体，比如“参数化ReLU”函数：
$$
pReLU(x) = max(0, x) + α min(0, x)
$$
该变体添加一个线性项，让某些负的信息可以通过

##### `sigmoid`函数

又称为“挤压函数”，作用是对于一个输入将其变换为区间（0，1）上的输出
$$
sigmoid(x) = \frac{1}{1 + e^{-x}}
$$

##### `tanh`函数

与`sigmoid`函数类似，`tanh`函数也能将输入压缩转换到区间（-1，1）上面，公式如下：
$$
tanh(x) = \frac{1 - e^{-2x}}{1 + e^{-2x}}
$$
该函数关于原点中心对称

### 多层感知机从0实现

注：在构建隐藏层时，常常将2的若干次幂作为层的宽度（便于硬件寻址）

```python
import torch
from torch import nn
from d2l import torch as d2l
import matplotlib.pyplot as plt

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

# 初始化模型参数
num_inputs, num_outputs, num_hiddens = 784, 10, 256
W1 = nn.Parameter(torch.randn(num_inputs, num_hiddens, requires_grad = True) * 0.01)
# Parameter将张量包装为PyTorch参数，自动注册到模型的参数列表中，使其可以被优化器识别更新
b1 = nn.Parameter(torch.zeros(num_hiddens, requires_grad = True))
W2 = nn.Parameter(torch.randn(num_hiddens, num_outputs, requires_grad = True) * 0.01)
b2 = nn.Parameter(torch.zeros(num_outputs, requires_grad = True))

params = [W1, b1, W2, b2]

# 激活函数（实现ReLU函数）
def relu(X):
    a = torch.zeros_like(X)
    return torch.max(X, a)

# 模型
def net(X):
    X = X.reshape((-1, num_inputs))
    H = relu(X@W1 + b1) # 此处的@表示矩阵乘法，等同于torch.matmul()
    return (H@W2 + b2)

# 损失函数
loss = nn.CrossEntropyLoss(reduction = 'none')

# 训练
num_epochs, lr = 10, 0.1
updater = torch.optim.SGD(params, lr = lr)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, updater)

d2l.predict_ch3(net, test_iter)

plt.show()
```

### 多层感知机简洁实现

代码如下

```py
import torch
from torch import nn
from d2l import torch as d2l
import matplotlib.pyplot as plt

net = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(), nn.Linear(256, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std = 0.01)

net.apply(init_weights)

batch_size, lr, num_epochs = 256, 0.1, 10
loss = nn.CrossEntropyLoss(reduction = 'none')
trainer = torch.optim.SGD(net.parameters(), lr = lr)

train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)

plt.show()
```

### 模型选择与欠/过拟合

#### 误差分类

- 训练误差：模型在训练计算集上得到的误差
- 泛化误差：模型在无限多样本中误差的期望（现实中理解为模型在测试集上的误差）

> “从特殊到一般”，这还是我初中数学老师天天念叨的话，真的很好

独立同分布假设：假设训练数据与测试数据都是从相同的分布中独立提取（但现实常常违背这个假设）

#### 过拟合与欠拟合

- 过拟合：个人理解为模型记住训练集样本特征与标签对应关系，而非找到一种普适的模型参数，从而导致在真实/测试数据集上表现不佳、与训练集差距极大
  - 一般表现为训练误差明显低于验证误差（但过拟合并不一定总是一件坏事）

- 欠拟合：没有训练到位，或者模型选择错误导致使用误差很大
  - 一般表现为训练与验证误差都很严重，但这两种误差间只有一点差距


#### 影响模型泛化的因素

- 参数数量（自由度）：一般来说，自由度越大（通常象征模型更复杂）更容易过拟合
- 参数取值范围
- 训练样本数量：个人认为与模型复杂度是相对而言，模型越复杂需要的样本也就越多

#### 数据集分类

细分为训练集、验证集、测试集

原因是希望只在最后测试时使用一遍测试集，但数据往往不够。常见方法是将数据分为三份，在训练和测试数据集外增加一个验证数据集。实际上验证与测试数据集的界限一般很模糊（一般将每一轮的test称为验证数据集而不是测试数据集）

**K折交叉验证：**

训练数据稀缺的时候可能无法提供足够的数据来构成一个合适的验证集。于是将原始训练数据分为K个不重叠的子集，每次在 K-1 个子集上训练，在剩余的子集中验证（共K次训练与验证）最后通过K次结果取平均估计训练 / 验证误差

#### 代码

```python
import math
import numpy as np
import torch
from torch import nn
from d2l import torch as d2l
import matplotlib.pyplot as plt

# 生成多项式数据集：100个训练，100个测试
max_degree = 20
n_train, n_test = 100, 100
true_w = np.zeros(max_degree)
true_w[:4] = np.array([5, 1.2, -3.4, 5.6])

features = np.random.normal(size = (n_train + n_test, 1)) # 生成两百个样本中的x
np.random.shuffle(features)
# 下一句使用arange而非range是因为range返回迭代器不能直接用于Numpy数组运算，而arange是Numpy函数
poly_features = np.power(features, np.arange(max_degree).reshape(1, -1)) # 计算每个多项式中每个项（除常数项）的结果
for i in range(max_degree): 
    poly_features[:, i] /= math.gamma(i + 1) # 200个多项式中每个项除以其幂数的阶乘（防止过大）
labels = np.dot(poly_features, true_w) # 每个项乘上其系数
labels += np.random.normal(scale = 0.1, size = labels.shape) # 每个多项式加上常数项
# NumPy ndarray转换为tensor
true_w, features, poly_features, labels = [torch.tensor(x, dtype = torch.float32) for x in [true_w, features, poly_features, labels]]

# 模型训练与测试
def evaluate_loss(net, data_iter, loss):
    """评估给定数据集上模型的损失"""
    metric = d2l.Accumlator(2) # 两个位置存放 损失的总和，样本数量
    for X, y in data_iter:
        out = net(X)
        y = y.reshape(out.shape)
        l = loss(out, y)
        metric.add(l.sum(), l.numel())
    return metric[0] / metric[1]


def train(train_features, test_features, train_labels, test_labels, num_epochs = 400):
    """训练函数"""
    loss = nn.MSELoss(reduction = 'none')
    input_shape = train_features.shape[-1] # 获取该张量最后一个维度的值
    net = nn.Sequential(nn.Linear(input_shape, 1, bias = False)) # bias表示是否有截距项
    batch_size = min(10, train_labels.shape[0])
    train_iter = d2l.load_array((train_features, train_labels.reshape(-1, 1)), batch_size)
    test_iter = d2l.load_array((train_features, test_labels.reshape(-1, 1)), batch_size, is_train = False)
    trainer = torch.optim.SGD(net.parameters(), lr = 0.01)
    animator = d2l.Animator(xlabel = 'epoch', ylabel = 'loss', yscale = 'log', xlim = [1, num_epochs], ylim = [1e-3, 1e2], legend = ['train', 'test'])
    for epoch in range(num_epochs):
        d2l.train_epoch_ch3(net, train_iter, loss, trainer)
        if epoch == 0 or (epoch + 1) % 20 == 0:
            animator.add(epoch + 1, (evaluate_loss(net, train_iter, loss)), evaluate_loss(net, test_iter, loss)) # 每隔一段训练记录训练次数与训练集 / 测试集上的损失数量，便于图像观察
    print('weight:', net[0].weight.data.numpy()) # 打印完成全部训练后得到的权重

# 从多项式特征中选择前4个维度，即1,x,x^2/2!,x^3/3!
train(poly_features[:n_train, :4], poly_features[n_train:, :4],
      labels[:n_train], labels[n_train:])

# 从多项式特征中选择前2个维度，即1和x
train(poly_features[:n_train, :2], poly_features[n_train:, :2],
      labels[:n_train], labels[n_train:])

# 从多项式特征中选取所有维度
train(poly_features[:n_train, :], poly_features[n_train:, :],
      labels[:n_train], labels[n_train:], num_epochs=1500)

```

**注意：**以上程序只能通过表现欠拟合（选择两个维度使用线性回归而非计算出多项式回归） / 过拟合（选取二十个维度导致噪声太多）来表示模型选择等问题导致的欠 / 过拟合问题的表现，而非说哪个模型更好

### 权重衰减

**问题：**如何缓解过拟合？在更多的训练数据样本难以获得的情况下，需要引入 正则化 技术来缓解过拟合的问题

#### 正则化

一种防止模型在训练过程中变的过于复杂乃至“记忆”训练样本导致过拟合的手段

此处表现为在损失函数中额外添加一个**惩罚项**，来约束模型的复杂性（鼓励其变得简单）。该惩罚项会惩罚模型复杂性，比如权重过大

**权重衰减：**也叫 L2正则化 ，将权重的L2范数作为惩罚项加入损失函数中，防止权重过大导致模拟复杂出现过拟合

```py
import torch
from torch import nn
from d2l import torch as d2l
import matplotlib.pyplot as plt

n_train, n_test, num_inputs, batch_size = 20, 100, 200, 5 # 为了使过拟合的效果更加明显，使用20个样本的小训练集
true_w, true_b = torch.ones((num_inputs, 1))*0.01, 0.05
train_data = d2l.synthetic_data(true_w, true_b, n_train)
train_iter = d2l.load_array(train_data, batch_size)
test_data = d2l.synthetic_data(true_w, true_b, n_test)
test_iter = d2l.load_array(test_data, batch_size, is_train = False)

# 初始化模型参数
def init_params():
    w = torch.normal(0, 1, size = (num_inputs, 1), requires_grad = True)
    b = torch.zeros(1, requires_grad = True)
    return [w, b]

# L2范数惩罚
def l2_penalty(w):
    return torch.sum(w.pow(2)) / 2

# 训练代码
def train(lambd):
    w, b = init_params()
    net, loss = lambda X: d2l.linreg(X, w, b), d2l.squared_loss
    # 此处第一个用匿名函数是由于具有动态参数需要重新绑定，而第二个不使用则是由于没有动态参数
    num_epochs, lr = 100, 0.003
    animator = d2l.Animator(xlabel = 'epochs', ylabel = 'loss', yscale = 'log', xlim = [5, num_epochs], legend = ['train', 'test'])
    for epoch in range(num_epochs):
        for X, y in train_iter:
            # 增加L2范数惩罚项
            l = loss(net(X), y) + lambd * l2_penalty(w)
            l.sum().backward()
            d2l.sgd([w, b], lr, batch_size)
        if (epoch + 1) % 5 == 0:
            animator.add(epoch + 1, (d2l.evaluate_loss(net, train_iter, loss), d2l.evaluate_loss(net, test_iter, loss)))
    print('w的L2范数是：',torch.norm(w).item())

# 忽略正则化直接训练
train(lambd = 0)
# 加入权重衰减考虑
train(lambd = 3)

plt.show()

# 简洁实现
def train_concise(wd):
    net = nn.Sequrntial(nn.Linear(num_inputs, 1))
    for param in net.parameters():
        param.data.normal_()
    loss = nn.MSELoss(reduction = 'none')
    num_epochs, lr = 100, 0.003
    # 偏置参数不设置衰减
    trainer = torch.optim.SGD([
        {"params": net[0].weight, 'weight_decay': wd},
        {"params": net[0].bias}
    ], lr = lr)
    animator = d2l.Animator(xlabel = 'epochs', ylabel = 'loss', yscale = 'log', xlim = [5, num_epochs], legend = ['train', 'test'])
    for epoch in range(num_epochs):
        for X, y in train_iter:
            trainer.zero_grad()
            l = loss(net(X), y)
            l.mean().backward()
            trainer.step()
        if (epoch + 1) % 5 == 0:
            animator.add(epoch + 1,(d2l.evaluate_loss(net, train_iter, loss), d2l.evaluate_loss(net, test_iter, loss)))
    print('w的L2范数：', net[0].weight.norm.item())
```

以上代码的结果表示，随着训练次数的增加，（在损失函数中是否使用正则化）训练集上的损失最终都会趋于一个低值，但在使用正则化的情况下测试集的损失函数会大大降低、与训练集的损失差距更小

### 暂退法

**问题：**权重衰减可以防止权重过大导致的模型复杂带来的过拟合；但在刚刚讨论的过程中只考虑了线性回归（没有考虑各个特征之间的内在联系），而在神经网络中同样有可能过拟合

**神经网络的过拟合表现：**记住训练集中一些非常特定，但是实际上无关紧要的模式组合（一个神经元在另一个非常特定的神经元也激活时才会起效）比如，一个神经网络要识别猫，可能学会依赖一个“发现特定款式的蝴蝶结”的神经元和一个“发现特定品种的胡须”的神经元同时出现，才判断为猫（因为训练集中大部分都是如此）

#### 暂退法

在神经网络计算后续层之前向前面每一层注入噪声；

具体表现为每一次计算前随机丢弃之前网络中的某一个或者某几个神经元，这样就可以使输出的计算不再依赖于这几个神经元；

随机丢弃多次来让各个神经元与更多不同的神经元简历泛化的连接

**注：**通常在测试时不使用暂退法；当然也存在例外，比如通过许多不同暂退法遮盖后的预测结果保持一致，可以说网络发挥稳定

#### 代码

```python
import torch
from torch import nn
from d2l import torch as d2l
import matplotlib.pyplot as plt

def dropout_layer(X, dropout):
    assert 0 <= dropout <= 1
    # 所有元素都被丢弃的情况
    if dropout == 1:
        return torch.zeros(X)
    # 所有元素都被保留的情况
    if dropout == 0:
        return X
    mask = (torch.rand(X.shape) > dropout).float() 
    # 在原样本同样分布的张量中，[0, 1)间生成均匀分布随机数与丢弃概率dropout比较，得到掩码矩阵，其中的项只有1.0 / 0.0 两种可能
    return mask * X / (1.0 - dropout)
    # 原样本与掩码矩阵相乘来随机丢弃。丢弃完成后需要进行缩放（因为我们丢弃了dropout比例的神经元，训练时的强度就会变成正常强度的1 / 1-dropout倍）

X = torch.arange(16, dtype = torch.float32).reshape((2, 8))

# 使用之前的Fashion-MNIST数据集，定义两个隐藏层多层感知机，每个隐藏层256个单元
num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256

# 定义模型
# 将暂退法应用于隐藏层的输出（激活函数）之后，为每一层设置暂退概率：通常靠近输入层的地方暂退概率较低
dropout1, dropout2 = 0.2, 0.5

class Net(nn.Module): # 继承自nn.Module类
    def __init__(self, num_inputs, num_outputs, num_hiddens1, num_hiddens2, is_training = True):
        super(Net, self).__init__() # 调用父类的构造函数（第一个参数指定要丛哪个子类开始查找父类，第二个参数为使用哪个子类实例）
        self.num_inputs = num_inputs # PY中类的字段不需要提前声明
        self.training = is_training
        self.lin1 = nn.Linear(num_inputs, num_hiddens1) # 返回一个模块对象（独属于PyTorch框架的类型）
        self.lin2 = nn.Linear(num_hiddens1, num_hiddens2)
        self.lin3 = nn.Linear(num_hiddens2, num_outputs)
        self.relu = nn.ReLU()

    def forward(self, X):
        H1 = self.relu(self.lin1(X.reshape((-1, self.num_inputs))))
        # 只有训练模型的时候才使用dropout
        if self.training == True:
            # 在第一个全连接层后添加一个dropout层
            H1 = dropout_layer(H1, dropout1)
        H2 = self.relu(self.lin2(H1))
        if self.training == True:
            # 第二个全连接层后添加一个dropout层
            H2 = dropout_layer(H2, dropout2)
        out = self.lin3(H2)
        return out

net = Net(num_inputs, num_outputs, num_hiddens1, num_hiddens2)

# 训练与测试
num_epochs, lr, batch_size = 10, 0.5, 256
loss = nn.CrossEntropyLoss(reduction = 'none')
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
trainer = torch.optim.SGD(net.parameters(), lr = lr)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)

plt.show()

# 简洁实现
# 在高级API中，只需要在每个全连接层后添加一个Dropout层，将暂退概率作为参数传给其构造函数即可
# 在训练时Dropout层将根据指定概率随机丢弃上一层输出；测试时Dropout层仅传递数据
net = nn.Sequential(nn.Flatten(), nn.Linear(784, 256), nn.ReLU(), nn.Dropout(dropout1), nn.Linear(256, 256), nn.ReLU(), nn.Dropout(dropout2), nn.Linear(256, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std = 0.01)

net.apply(init_weights);

trainer = torch.optim.SGD(net.parameters(), lr=lr)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

以上代码仅说明 暂退法 的实现原理与API实现方法，并未与未实现暂退法的模型进行比较

### 数值稳定性与模型初始化

#### 梯度消失与梯度爆炸

在一个神经网络中，我们使用计算图记录从输入层到输出层计算的前向传播路径；在训练需要优化参数时，使用反向传播从输出层到输入层逐级调整参数

**问题：**根据链式法则的计算方法，求复合函数的导数是其中各个函数的导数相乘。那么在反向传播时，如果前面每一层的导数都很小，那么靠近输入层的网络算出的导数会趋近于0无法调整；在前向传播时，如果靠近输入层每一层的导数都很大，最后算出来的导数会非常大。这就是梯度消失与梯度爆炸问题

**梯度消失**

举例：使用`sigmoid`函数作为激活函数
$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$
求导可得，该函数的导数 ≤ 0.25；而且很有可能接近于0，会在某一层切断梯度

因此通常使用更稳定的`ReLU`系列函数

#### 模型初始化

**对称性**

对称性：一种模型有多种最优解。比如将同一层神经元位置交换的同时把对应的参数与神经元全部交换，得到的依然是最优解（哪怕参数和之前的完全不同）

**问题：**假设一个非常简易的神经网络，仅有一个隐藏层与两个隐藏单元。如果将隐藏层的所有参数初始化为常量c，会发现在一条传播路径上做一模一样处理的模型导致这些参数的优化永远一致，这些参数也永远相同。因此我们需要在初始化时要给予参数一个倾向（倾向于某一种最优解）

**解决方法：**我们在之前使用正态分布的随机初始化。除此之外还有`Xavier`初始化等等十余种不同的启发式初始化方法



### 分布偏移

概念：分布偏移指的是，模型训练时使用的数据分布与实际应用时遇到的数据分布不一致的情况

在下面介绍各个偏移时，y都指的是标签 / 输出, X指的是某一个特征 / 输入

#### 协变量偏移

**协变量：**原意指的是“伴随变量”或者“控制变量”，此处指机器学习中出现在输入中、但是不应该影响输出（训练者不希望影响，但是有可能会在学习中被记忆）的特征

比如：在分辨猫狗的模型中，背景的亮度等等就应该是协变量

**协变量偏移：**输入的分布改变，但标签函数 P( y | X ) （已知X发生则y发生的概率）不变。比如辨别猫狗时，背景发生改变但判别猫狗的方法不应该发生改变

#### 标签偏移

**标签：**就是我们所说的标签，可以简单理解为输出的结果

**标签偏移：**P(y)改变，但是P( X | y )不变 即 Y的分布变了，但是每个Y对应的X特征没变

比如：在判断流感 / 感冒的模型中，流感和感冒的分布可能会随着时间的变化而变化，但是如果出现了流感，那么其中一直都有80%的可能性出现高烧这个特征

#### 概念偏移

**概念偏移：**当标签的定义发生变化时出现的问题

比如在训练时红灯停绿灯行，但是在测试时虽然还是红绿灯，但是红灯行
