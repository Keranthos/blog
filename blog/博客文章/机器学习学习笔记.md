### 线性回归

用于预测“多少”问题（线性连续），比如出售价格，胜场数量

#### 线性回归模型方程

核心思想：估计数据模型为`y = Xw + b`，给定所有样本数据与对应标签，通过不断从总样本中获取小批量样本（标签值），据此通过梯度下降不断修正参数，最终找到合适的参数值

#### 从零实现代码部分

```python
import random
import torch
from d2l import torch as d2l

# 生成样本
def synthetic_data(w, b, num_examples):
    X = torch.normal(0, 1, (num_examples, len(w)))
    y = torch.matmul(X, w) + b
    y += torch.normal(0, 0.01, y.shape)
    return X, y.reshape((-1, 1))

true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = synthetic_data(true_w, true_b, 1000);

'''
print('features:', features[0],'\nlabel:', labels[0])

d2l.set_figsize()
d2l.plt.scatter(features[:, (1)].detach().numpy(), labels.detach().numpy(), 1)
d2l.plt.show()
'''

# 小批量获得样本
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    random.shuffle(indices)
    for i in range(0, num_examples, batch_size):
        batch_indices = torch.tensor(indices[i: min(i + batch_size, num_examples)])
        yield features[batch_indices], labels[batch_indices]

'''
batch_size = 10
for X, y in data_iter(batch_size, features, labels):
    print(X, '\n', y)
    break
'''

# 开始进行模型构建
w = torch.normal(0, 0.01, size = (2, 1), requires_grad = True)
b = torch.zeros(1, requires_grad = True)

def linreg(X, w ,b):
    """线性回归模型"""
    return torch.matmul(X, w) + b

def squared_loss(y_hat, y):
    """均方损失"""
    return (y_hat - y.reshape(y_hat.shape))**2 / 2

def sgd(params, lr, batch_size):
    """参数调整"""
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()

# 训练
batch_size = 10
lr = 0.03
num_epochs = 3
net = linreg
loss = squared_loss

for epoch in range(num_epochs):
    for X,y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y)
        l.sum().backward()
        sgd([w, b], lr, batch_size)
    with torch.no_grad():
        train_l = loss(net(features, w, b), labels)
        print(f'epoch{epoch + 1}, loss{float(train_l.mean()):f}')
```

#### API实现代码部分

```python
import torch
import numpy as np
from torch.utils import data
from d2l import torch as d2l

true_w = torch.tensor([2, -3.4])
true_b = 4.2
features, labels = d2l.synthetic_data(true_w, true_b, 1000)

def load_array(data_arrays, batch_size, is_train = True):
    dataset = data.TensorDataset(*data_arrays)
    return data.DataLoader(dataset, batch_size, shuffle = is_train)

batch_size = 10
data_iter = load_array((features, labels), batch_size)

from torch import nn
net = nn.Sequential(nn.Linear(2, 1))

net[0].weight.data.normal_(0, 0.01)
net[0].bias.data.fill_(0)

loss = nn.MSELoss()

trainer = torch.optim.SGD(net.parameters(), lr = 0.03)

num_epochs = 3
for epoch in range(num_epochs):
    for X, y in data_iter:
        l = loss(net(X), y)
        trainer.zero_grad()
        l.backward()
        trainer.step()
    l = loss(net(features), labels)
    print(f'epoch {epoch + 1}, loss {l:f}')

```

### `softmax`回归

用于预测分类问题，即判断样本属于哪一个类型，标签为类型（离散型）

连续的线性回归预测仅需要一个线性方程`y = Xw + b`（y为一维张量）

`softmax`回归需要n个，n为可能的类型数，每个类型都有一个线性方程`yn = Xw + b`，返回该样本中该类型可能性的大小（不是概率，因为不一定属于0-1之间，通过与同一样本其他类型的可能性大小比较）

**独热编码：**一种将分类变量转换为机器学习算法更容易理解的形式的方法，具体而言就是将具有 `K` 个类别的特征转换为 `K` 个二进制特征，每个特征表示一个类别是否存在

#### `softmax`模型方程

预测类型的方程为`y = Xw + b`（y为n维张量，代表该样本中各个类型的可能性大小）

#### `softmax`处理

由于计算出y的各个值是大小而非概率，所以需要将其转化为概率
$$
\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}} \quad \text{对于 } i = 1, 2, \dots, K
$$

即计算该样本各个类型的可能性大小的exp()之和作为分母，每个类型可能性大小的exp()分别作为分子

#### 交叉熵

在类型判断问题中计算损失大小的方法，

最简单的是`-log(该样本真正类别在预测中的可能性大小)`（根据图象，样本真正标签在预测中的可能性大小越接近于1，则log()函数值在x轴下方越逼近于0，损失函数也就越小）
$$
\mathcal{L}_{\text{CE}} = -\frac{1}{N} \sum_{i=1}^{N} \sum_{j=1}^{K} y_{i,j} \log(\hat{y}_{i,j})
$$

该计算方法适用于多标签分类问题（即一个样本可能有几个标签、属于几个类型）

N为样本数量，K为类别数量
$$
\mathcal{L}_{\text{CE}} = -\frac{1}{N} \sum_{i=1}^{N} \log(\hat{y}_{i, y_i})
$$
单标签分类问题的简化公式：将每个样本的损失函数求和取均值即可

#### `FashionMNIST`图像集获取

```python
import torch
import torchvision
from torch.utils import data
from torchvision import transforms
from d2l import torch as d2l

# 设置图片为矢量图，显示更加清晰
d2l.use_svg_display()

# 下载与预处理FashionMNIST数据
trans = transforms.ToTensor() # 由（高度， 宽度， 通道）变为（通道， 高度， 宽度），处理器处理（将所有像素的同一个通道当作一个整体来处理）时更加高效、访存更快速
mnist_train = torchvision.datasets.FashionMNIST(
    root = "../data", train = True, transform = trans, download = True
)
mnist_test = torchvision.datasets.FashionMNIST(
    root = "../data", train = False, transform = trans, download = True
)
'''
len(mnist_train), len(mnist_test)
mnist_train[0][0].shape
'''

def get_fashion_mnist_labels(labels):
    """返回Fashion-MNIST数据集的文本标签"""
    text_labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']
    return [text_labels[int(i)] for i in labels]

def show_images(imgs, num_rows, num_cols, titles = None, scale = 1.5): # 在网格中显示多张图片
    """绘制图像列表"""
    figsize = (num_cols * scale, num_rows * scale) # 计算整个网格的尺寸；scale为缩放系数，控制图片显示大小
    _, axes = d2l.plt.subplots(num_rows, num_cols, figsize = figsize)
    axes = axes.flatten() # 将[num_row * num_cols]二维网格展平为一维列表
    for i, (ax, img) in enumerate(zip(axes, imgs)):
        if torch.is_tensor(img):
            ax.imshow(img.numpy())
        else:
            ax.imshow(img) # 显示图片
        ax.axes.get_xaxis().set_visible(False) # 隐藏X轴
        ax.axes.get_yaxis().set_visible(False) # 隐藏Y轴
        if titles:
            ax.ser_title(titles[i]) # 设置标题
    return axes
'''
X, y = next(iter(data.DataLoader(mnist_train, batch_size=18)))
show_images(X.reshape(18, 28, 28), 2, 9, titles=get_fashion_mnist_labels(y));
'''

# 读取小批量数据

batch_size = 256

def get_dataloader_workers():
    """使用4个进程来读取数据"""
    return 4

train_iter = data.DataLoader(mnist_train, batch_size, shuffle = True, num_workers = get_dataloader_workers())

'''
timer = d2l.Timer()
for X, y in train_iter:
    continue
print(f'{timer.stop():.2f} sec')
'''

def load_data_fashion_mnist(batch_size, resize=None):  
    """下载Fashion-MNIST数据集，然后将其加载到内存中"""
    trans = [transforms.ToTensor()] # list类型，表示转化的步骤
    if resize: # 是否将resize(重置图片大小)操作放在list中，ToTensor之前
        trans.insert(0, transforms.Resize(resize))
    trans = transforms.Compose(trans) # 将trans这个list变成一个Compose对象（可执行流水线），交给FashionMNIST
    mnist_train = torchvision.datasets.FashionMNIST(
        root="../data", train=True, transform=trans, download=True)
    mnist_test = torchvision.datasets.FashionMNIST(
        root="../data", train=False, transform=trans, download=True)
    return (data.DataLoader(mnist_train, batch_size, shuffle=True,
                            num_workers=get_dataloader_workers()),
            data.DataLoader(mnist_test, batch_size, shuffle=False,
                            num_workers=get_dataloader_workers()))

'''
train_iter, test_iter = load_data_fashion_mnist(32, resize=64)
for X, y in train_iter:
    print(X.shape, X.dtype, y.shape, y.dtype)
    break
'''
```

#### 从0实现代码部分

```python
import torch
from IPython import display
from d2l import torch as d2l
import matplotlib.pyplot as plt

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

# 初始化：每个图像都是28*28，将其展平视为长度784的向量（每个位置作为一个特征）
num_inputs = 784 # 权重行数，等于样本特征数
num_outputs = 10 # 权重列数，等于输出维度（最终有多少种类别）

w = torch.normal(0, 0.01, size = (num_inputs, num_outputs), requires_grad = True)
b = torch.zeros(num_outputs, requires_grad = True) # 偏置b为1*num_outputs

# softmax操作
def softmax(X):
    X_exp = torch.exp(X) # 样本*特征 矩阵中每个值求幂
    partition = X_exp.sum(1, keepdim = True) # 对每一行（一个样本各个类型的可能值）求和
    return X_exp / partition

# 模型定义
def net(X):
    return softmax(torch.matmul(X.reshape((-1, w.shape[0])), w) + b)

# 损失函数
def cross_entropy(y_hat, y):
    return - torch.log(y_hat[range(len(y_hat)), y]) 
'''
高级索引y_hat[range(len(y_hat)), y]，range(len(y_hat))表示遍历y_hat每个样本，y表示这个样本的正确类型
返回包含每个样本正确类型的概率组成的一维张量
比如：y_hat 是二维张量，形状为 [批量大小, 类别数]
y_hat = torch.tensor([[0.1, 0.3, 0.6],    # 样本1的3个类别概率
                      [0.4, 0.2, 0.4],    # 样本2的3个类别概率  
                      [0.7, 0.1, 0.2]])   # 样本3的3个类别概率
y = torch.tensor([2, 0, 1])  # 真实标签，形状: [3]
则y_hat[range(len(y_hat)), y]等于y_hat[[0, 1, 2], [2, 0, 1]]
'''
'''
cross_entropy(y_hat, y)
'''

# 分类精度（一个批量中预测正确的比例）
def accuracy(y_hat, y):
    """计算预测正确的数量"""
    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1: #y_hat的第二维度中存储样本每个类型的预测概率
        y_hat = y_hat.argmax(axis = 1) # 将y_hat每行的值变成获取每个样本预测概率最大的类别索引
    cmp = y_hat.type(y.dtype) == y # 将y_hat与y逐个项对比，返回True / False
    return float(cmp.type(y.dtype).sum()) # 返回正确预测的样本数量

def evaluate_accuracy(net, data_iter):
    """计算指定数据集上模型精度"""
    if isinstance(net, torch.nn.Module): # 检查net是否torch中的模型类型
        net.eval() # 将模型设置为评估模式
    metric = Accumulator(2) 
    with torch.no_grad():
        for X,y in data_iter:
            metric.add(accuracy(net(X), y), y.numel()) # 最后一个样本数可能不同，将一个批量样本中正确的数量与样本大小分别存入metric
    return metric[0] / metric[1]

class Accumulator:
    """用于对多个变量进行累加"""
    def __init__(self, n):
        self.data = [0.0] * n

    def add(self, *args):
        self.data = [a + float(b) for a, b in zip(self.data, args)]
        # zip(self.data, args)将self.data和传入的参数一一配对

    def reset(self):
        self.data = [0, 0] * len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]

# 训练
def train_epoch_ch3(net, train_iter, loss, updater):
    """训练模型一个迭代周期"""
    if isinstance(net, torch.nn.Module):
        net.train() # 设置为训练模式
    metric = Accumulator(3) # 训练损失总和、训练准确度总和、样本数
    for X, y in train_iter:
        # 计算梯度、更新参数
        y_hat = net(X)
        l = loss(y_hat, y)
        if isinstance(updater, torch.optim.Optimizer):
            # 使用torch内置的优化器与损失函数
            updater.zero_grad()
            l.mean().backward()
            updater.step()
        else:
            # 定制优化器与损失函数
            l.sum().backward()
            updater(X.shape[0])
        metric.add(float(l.sum()), accuracy(y_hat, y), y.numel())
    return metric[0] / metric[2], metric[1] / metric[2]

# 此处定义了一个在动画中绘制数据的类Animator，略

def train_ch3(net, train_iter, test_iter, loss, num_epochs, updater):
    """训练模型"""
    animator = d2l.Animator(xlabel = 'epoch', xlim = [1, num_epochs], ylim = [0.3, 0.9], legend = ['train loss', 'train acc', 'test acc'])
    for epoch in range(num_epochs):
        train_metrics = train_epoch_ch3(net, train_iter, loss, updater)
        test_acc = evaluate_accuracy(net, test_iter)
        animator.add(epoch + 1, train_metrics + (test_acc, ))
    train_loss, train_acc = train_metrics
    # assert train_loss < 0.5, train_loss
    # assert train_acc <= 1 and train_acc > 0.7, train_acc
    # assert test_acc <= 1 and test_acc > 0.7, test_acc

lr = 0.1

def updater(batch_size):
    return d2l.sgd([w, b], lr, batch_size)

# 预测
def predict_ch3(net, test_iter, n = 6):
    """可视化模型预测结果，显示真实与预测标签"""
    for X, y in test_iter:
        break # 只获取第一批test数据
    trues = d2l.get_fashion_mnist_labels(y) # 将数字标签转换为文本描述
    preds = d2l.get_fashion_mnist_labels(net(X).argmax(axis = 1)) # 获得每个样本各个类型的预测概率，并且找到最大值，转化为文本描述
    titles = [true + '\n' + pred for true, pred in zip(trues, preds)]
    d2l.show_images(X[0:n].reshape((n, 28, 28)), 1, n, titles = titles[0:n])

if __name__ == '__main__':
    num_epochs = 10
    train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, updater)
    predict_ch3(net, test_iter)
    plt.show()
```

#### API实现代码部分

```python
import torch
from torch import nn
from d2l import torch as d2l

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

# PyTorch不会隐式地调整输入的形状。因此在线性层前定义了展平层（flatten），来调整网络输入的形状
# 期望的输入形状是(batch_size, 784),因为线性层是（784，10）输入为784输出为10，而实际上的输入是(btach_size, 1, 28, 28)
net = nn.Sequential(nn.Flatten(), nn.Linear(784, 10))

def init_weights(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, std = 0.01)

net.apply(init_weights);
'''
# PyTorch内部大致是这样工作的：
def apply(self, fn):
    for module in self.modules():  # 遍历网络中的所有模块
        fn(module)                 # 对每个模块调用init_weights函数
    return self

# 所以当调用 net.apply(init_weights) 时：
# m 就是 net 中的每个子模块
'''

loss = nn.CrossEntropyLoss(reduction = 'none')
'''
此处的reduction = 'none'表示返回各个样本的损失值，二维张量（默认返回的是各个样本的损失值的平均值，一维张量）
CrossEntropyLoss函数将softmax与负对数似然结合，防止e的x次方（x为一个样本中判断为某个类型的概率，计算softmax时使用）
过大或者过小，导致上下溢出
'''

trainer = torch.optim.SGD(net.parameters(), lr = 0.1) # 学习率为0.1的小批量随机梯度下降作为优化算法

num_epochs = 10
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, trainer)
```

